{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3136,
          "databundleVersionId": 26502,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30028,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Titanic Survival Prediction using ANN ",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khuzaifa5188/Analyzing-Titanic-data-set-by-using-ANN/blob/main/Titanic_Survival_Prediction_using_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "DAXjJW5eIlIb"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "titanic_path = kagglehub.competition_download('titanic')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "vvgusYCSIlIe"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Titanic Survival Prediction\n",
        "The reason for this analysis and model is to predict wether or not a person on the titanic will survive or not based on various features such as age, class, sex and where they embarked."
      ],
      "metadata": {
        "id": "6FzQwCOwIlIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "9O7jM6LQIlIi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
        "test_set = pd.read_csv('/kaggle/input/titanic/test.csv')"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "y3GfUIfOIlIj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variable Identification\n",
        "First I will explore each variable first, I want to find out the data type of each and how many null entries I have in the dataset."
      ],
      "metadata": {
        "id": "ZdOXsCivIlIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "xJ6Ud9jmIlIk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am going to change the index to Passenger ID as this is the same as the index we have already making the column redundent."
      ],
      "metadata": {
        "id": "egCdTwaaIlIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = train_set.set_index('PassengerId')"
      ],
      "metadata": {
        "trusted": true,
        "id": "F5bWi1ExIlIl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ilkooKheIlIm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows we have a few text columns and quite a big handful of null entries. Also Cabin has a huge amount of null entires so I will have to do something about this."
      ],
      "metadata": {
        "id": "aylEXJi5IlIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.info()"
      ],
      "metadata": {
        "trusted": true,
        "id": "RjM3a_1EIlIn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.describe()"
      ],
      "metadata": {
        "trusted": true,
        "id": "U3Fc5jSKIlIn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate Analysis\n",
        "Now I will visualize some features to try and find some outliers and see if we can find some interesting stats."
      ],
      "metadata": {
        "id": "zV-zjIotIlIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(train_set['Pclass'].unique(), train_set['Pclass'].value_counts())"
      ],
      "metadata": {
        "trusted": true,
        "id": "g6_rK1lDIlIn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "bar = plt.bar(train_set['Sex'].unique(), train_set['Sex'].value_counts())\n",
        "bar[0].set_color('blue')\n",
        "bar[1].set_color('pink')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "JZEDFr4NIlIn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_set['Age'].hist()"
      ],
      "metadata": {
        "trusted": true,
        "id": "EP6hrqjpIlIo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bi-variate Analysis\n",
        "Now I will compare features against each other to try and find some correlation between them."
      ],
      "metadata": {
        "id": "VPBPa7htIlIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows that you are more likely to survive if you are a woman as they were sent of the ship first and over 5 times more likley to not survive if you are a man."
      ],
      "metadata": {
        "id": "0thcmA7QIlIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'Gender': train_set['Sex'], 'Survived': train_set['Survived']})\n",
        "total_counts = df.groupby(['Survived', 'Gender']).size()\n",
        "total_counts.plot.bar(rot=0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "TKY7bo_YIlIo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation_heatmap(train):\n",
        "    correlations = train.corr()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(16,16))\n",
        "    sb.heatmap(correlations, vmax=1.0, center=0, fmt='.2f', square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\":.70})\n",
        "    plt.show()\n",
        "correlation_heatmap(train_set)"
      ],
      "metadata": {
        "trusted": true,
        "id": "WRC5fkt4IlIo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Values\n",
        "Now I will treat the missing values by first removing the redundant columns like Name, Ticket and Cabin aswel as removing the little handful of null rows."
      ],
      "metadata": {
        "id": "_1FHTgyVIlIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def values_drop(set):\n",
        "    set = set.drop('Name', axis=1)\n",
        "    set = set.drop('Ticket', axis=1)\n",
        "    set = set.drop('Cabin', axis=1)\n",
        "    set = set.dropna()\n",
        "    return set\n",
        "def values_drop_test(set):\n",
        "    set = set.drop('Name', axis=1)\n",
        "    set = set.drop('Ticket', axis=1)\n",
        "    set = set.drop('Cabin', axis=1)\n",
        "    return set\n",
        "train_set = values_drop(train_set)\n",
        "test_set = values_drop_test(test_set)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9nrfaYNnIlIp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = test_set.replace(np.nan, 0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "G_x-D2EOIlIp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_set.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "8T_P7Pi5IlIp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode Categorical features\n",
        "Now I will use One Hot Encoding to chnage the Sex and Embarked columns to be continuous variables."
      ],
      "metadata": {
        "id": "jd-XASThIlIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "survived = train_set[['Survived']]\n",
        "train_set = train_set.drop(\"Survived\", axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "txVrMqB9IlIp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sex_cat = train_set[[\"Sex\"]]\n",
        "emb_cat = train_set[[\"Embarked\"]]"
      ],
      "metadata": {
        "trusted": true,
        "id": "PCDV2s-hIlIp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "OHE = OneHotEncoder()\n",
        "sex_cat_encoded = OHE.fit_transform(sex_cat)\n",
        "sex_cat_encoded.toarray()"
      ],
      "metadata": {
        "trusted": true,
        "id": "6Oby3pdzIlIq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "emb_cat_encoded = OHE.fit_transform(emb_cat)\n",
        "emb_cat_encoded.toarray()"
      ],
      "metadata": {
        "trusted": true,
        "id": "-gCQTfdSIlIq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_num = train_set.drop([\"Sex\", \"Embarked\"], axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "AHEB51FvIlIq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I create a full pipeline of transformations so I can easily call it on new entries and exisitng ones."
      ],
      "metadata": {
        "id": "FaQ0RSmkIlIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "num_attribs = list(train_num)\n",
        "cat_attribs = [\"Sex\", \"Embarked\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), num_attribs),\n",
        "    (\"cat\", OneHotEncoder(), cat_attribs)\n",
        "])\n",
        "train_prepared = full_pipeline.fit_transform(train_set)"
      ],
      "metadata": {
        "trusted": true,
        "id": "AAiDFiKlIlIq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_prepared = full_pipeline.fit_transform(test_set)"
      ],
      "metadata": {
        "trusted": true,
        "id": "AOn4hg9rIlIr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Models\n",
        "Now I will train a neural network and optimize it as best I can without overfitting and underfitting."
      ],
      "metadata": {
        "id": "cNe8zjejIlIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = train_prepared\n",
        "y = survived\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)"
      ],
      "metadata": {
        "trusted": true,
        "id": "lkJCLMduIlIr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "5lZiashAIlIr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_network():\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Dense(100, activation='relu', input_dim=10),\n",
        "        keras.layers.Dense(66, activation='relu'),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "def evaluate(model):\n",
        "    model.summary()\n",
        "    model.compile(optimizer=keras.optimizers.SGD(lr=0.05), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    history = model.fit(X_train, y_train, batch_size=40, epochs=30, validation_split=.1,\n",
        "                       callbacks=[keras.callbacks.EarlyStopping(patience=5)])\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['training', 'validation'], loc='best')\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "X4H7G92EIlIr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_network()\n",
        "evaluate(model)"
      ],
      "metadata": {
        "trusted": true,
        "id": "6sRVyjlHIlIr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This looks like it is fair on all three sets which is great news!"
      ],
      "metadata": {
        "id": "r4nkoKylIlI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_acc = model.evaluate(X_test, y_test)\n",
        "print(\" Model Accuracy is : {0:.1f}%\".format(model_acc[1]*100))"
      ],
      "metadata": {
        "trusted": true,
        "id": "7FP6lTzyIlI0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final predictions on the test set."
      ],
      "metadata": {
        "id": "de6g21vkIlI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.predict_classes(test_prepared)\n",
        "results = pd.Series(results[:,0], name=\"Survived\")\n",
        "submission = pd.concat([pd.Series(test_set.PassengerId, name=\"PassengerId\"),results],axis = 1)\n",
        "submission.to_csv('my_submission.csv', index=False)\n",
        "print(\"Your submission was successfully saved!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "4XFGMiRGIlI1"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}